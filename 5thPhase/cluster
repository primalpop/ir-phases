#!/usr/bin/python
#coding: utf8
"""
cluster

Homework 5: http://www.csee.umbc.edu/~nicholas/676/term%20project/hw5.html

Author: Primal Pappachan, primal1@umbc.edu

Usage: cluster <files>

"""

import nltk, unicodedata, HTMLParser
import os, sys, re
import collections
import random, math
from pprint import pprint
import operator

class Similarity:
    """Similarity Matrix for Document Clustering"""
    def __init__(self):
        self.sim_matrix = collections.defaultdict(dict) #document : document : cosine similarity
        self.active_clusters = {} #keeps track of active clusters
        self.cluster_info = {} # cluster : list of documents which are part of that cluster

    def documents(self, element):
        """If parameter is an integer, it is a cluster and document otherwise."""
        if isinstance(element, basestring):
            return [(element)]
        else:
            return self.cluster_info[element]

    def get_high_sim(self):
        """ Returns the maximum similarity score as a tuple (score, d1, d2) """
        flag = -1
        maximum = (flag, 0, 0)
        for d1 in self.sim_matrix.keys():
            for d2 in self.sim_matrix[d1].keys():
                if d1 != d2: #ignoring similarity with itself
                    if self.active_clusters[d1] != -1 and self.active_clusters[d2] != -1: #checking if cluster is active
                        score = self.sim_matrix[d1][d2]
                        if score > flag and score != 1:
                            flag = score
                            maximum = (flag, d1, d2)
        return maximum

    def get_lowest_sim(self):
        """ Returns the minimum similarity score in the corpus"""
        flag = 1
        maximum = (flag, 0, 0)
        for d1 in self.sim_matrix.keys():
            for d2 in self.sim_matrix[d1].keys():
                if d1 != d2: #ignoring similarity with itself
                    if self.active_clusters[d1] != -1 and self.active_clusters[d2] != -1: #checking if cluster is active
                        score = self.sim_matrix[d1][d2]
                        if score < flag:
                            flag = score
                            minimum = (flag, d1, d2)
        return minimum

    def get_high_sim_centroid(self, d1, cluster):
        """ Returns the maximum similarity score as a tuple (score, d1, d2) """
        flag = -1
        maximum = (flag, 0, 0)
        for d2 in self.cluster_info[cluster]:
            if d1 != d2: #ignoring similarity with itself
                #import ipdb; ipdb.set_trace()
                if self.sim_matrix[d1].has_key(d2):
                    score = self.sim_matrix[d1][d2]
                else:
                    score = self.sim_matrix[d2][d1]
                if score > flag and score != 1:
                    flag = score
                    maximum = (flag, d1, d2)
        return maximum

    def number_of_active_clusters(self):
        """Returns the number of active clusters"""
        return len([i for i, e in enumerate(self.active_clusters.values()) if e != -1])

    def group_link_avg(self, new_cluster, other_cluster):
        """Recompute distances for the similarity matrix based on the new cluster"""
        nc_num = self.active_clusters[new_cluster]
        oc_num = self.active_clusters[other_cluster]
        if isinstance(other_cluster, basestring):
            ocluster = [(other_cluster)]
        else:
            ocluster = self.cluster_info[other_cluster]
        before_avg = 0
        for y in ocluster:
            for x in self.cluster_info[new_cluster]:
                if self.sim_matrix[y].has_key(x):
                    before_avg += self.sim_matrix[y][x]
                else:
                    before_avg += self.sim_matrix[x][y]
        avg = before_avg / (nc_num + oc_num)
        return avg


class Index:
    """Variables for building Term Document Matrix"""
    def __init__(self):
        self.weights = collections.defaultdict(dict) #term : document : normalized term weight

    def cosine_denominator(self, doc):
        result = sum(v **2 for v in self.weights[doc].values())
        return math.sqrt(result)

    def product(self, doc1, doc2):
        keys = self.weights[doc1].viewkeys() &  self.weights[doc2].viewkeys()
        result = sum (self.weights[doc1][k] * self.weights[doc2][k] for k in keys)
        return result

class Calcwts:
    """ Methods and variables for term weight computation """
    def __init__(self):
        self.model = collections.defaultdict(dict) #document : term : frequency
        self.total_docs = 0 #Total Number of documents
        self.avg_doclen = 0 #Average length of documents
        self.inverse_doc_freq = {} #Inverse Document Frequency

    def freq(self, tokens):
        index = {}
        for (term, freq) in collections.Counter(tokens).iteritems():
            index[term] = freq
        return index

    def tf(self, term_freq, total_freq):
        """Computes term frequency"""
        return term_freq / float (total_freq)

    def idf(self, word):
        """ Computes Inverse Document frequency """
        return math.log(self.total_docs / float(self.inverse_doc_freq[word]))

    def tf_idf(self, word, doc, doclen):
        term_freq = self.model[doc][word]
        tf_value = self.tf(term_freq, doclen)
        idf_value = self.idf(word)
        return tf_value * idf_value

    def bm25_ranking(self, word, doc, doclen):
        """ Calculates BM25 with K1 = 1 and S1 = K1 + 1 """
        k1 = 1
        s1 = k1 + 1
        freq = self.model[doc][word]
        return s1 * (freq / ( ( (k1 * doclen ) / self.avg_doclen ) + freq ))

class Tokenization:
    """Methods and variables for tokenization"""
    def __init__(self):
        self.html_parser = HTMLParser.HTMLParser()
        self.tokenizer = nltk.tokenize.RegexpTokenizer("[\wâ€™]+", flags=re.UNICODE)
        self.stopwords = open('stoplist.txt').read().splitlines() #from file
        try:
            self.stopwords.extend(nltk.corpus.stopwords.words('english')) #from nltk corpus
        except Exception:
            pass

    def clean_html(self, raw):
        """
        Clean the raw string.
        It removes the html markup, formatting as well as non alphanumeric characters.
        Return string.
        """
        cleaned = nltk.clean_html(raw)
        cleaned = cleaned.decode('utf8', 'ignore')
        cleaned = self.html_parser.unescape(cleaned)
        cleaned = unicodedata.normalize('NFKD', cleaned).encode('latin-1','ignore')
        return cleaned

    def extract_tokens(self, cleaned):
        """
        Converts cleaned text to a vocabulary.
        Returns dictionary.
        """
        text = self.tokenizer.tokenize(cleaned)
        #Limiting words to have size > 2 and < 20 and not numbers
        words = [w.lower() for w in text if len(w) > 2 and len(w) < 20 and not w.isdigit()]
        #Stopwords removal
        words = [word for word in words if word not in self.stopwords]
        return words

def write_to_file(output, filename):
    """ Writes into an output file. """
    with open(filename, "w") as f:
        f.write(output)

def median(mylist):
    """ Finds median of the list
    Ref: http://stackoverflow.com/questions/10482339/how-to-find-median"""
    sorts = sorted(mylist)
    length = len(sorts)
    if not length % 2:
        return (sorts[length / 2] + sorts[length / 2 - 1]) / 2.0
    return sorts[length / 2]

def main(args):
    input_path= args
    number_of_files = 503

    tkr = Tokenization()

    list_of_files = []
    for (dirpath, dirnames, filenames) in os.walk(input_path):
        for filename in filenames:
            list_of_files.append(os.sep.join([dirpath, filename])) #List of complete files

    print "Phase 1: Starting tokenization"
    cwts = Calcwts()
    for i in range(0, number_of_files): # Slicing the list of files as per Number of files
        filename = list_of_files[i]
        file_obj = open(filename)
        raw_string = file_obj.read()
        cleaned = tkr.clean_html(raw_string) #cleaning raw html string
        tokens = tkr.extract_tokens(cleaned) #tokenization
        token_frequency = cwts.freq(tokens) #computing frequency of extracted tokens
        for token in tokens:
            cwts.model[filename][token] = token_frequency[token]
    print "Tokenization completed"

    vocabulary = []
    for docs in cwts.model.values():
        vocabulary.extend(docs.keys()) #complete vocabulary with repetitions; for computing document frequency in idf
    cwts.inverse_doc_freq = cwts.freq(vocabulary)

    print "Phase 2: Term Weighting"
    cwts.total_docs = len(cwts.model.keys())
    for d in cwts.model.keys():
        cwts.avg_doclen += sum(cwts.model[d].values())
    cwts.avg_doclen /= float(cwts.total_docs) #average document length

    inv_index = Index()
    for i in range(0, number_of_files):
        doc = list_of_files[i]
        doclen = sum(cwts.model[doc].values()) #length of document
        for word in cwts.model[doc].keys():
            term_weight = "%.8f" %(cwts.tf_idf(word, doc, doclen))
            inv_index.weights[doc][word] = eval(term_weight) #Creating the [term][document] weight dictionary
    print "Term Weighting completed"

    print "Phase 5: Computing Similarity scores"
    #Building singleton clusters based on cosine similarity
    SimObj = Similarity()
    for i in range(0, number_of_files):
        doc1 = list_of_files[i]
        denominator_doc1 = inv_index.cosine_denominator(doc1)
        for j in range(i, number_of_files): #making it upper triangular
            doc2 = list_of_files[j]
            denominator_doc2 = inv_index.cosine_denominator(doc2)
            SimObj.sim_matrix[doc1][doc2] = inv_index.product(doc1, doc2) / (denominator_doc1 * denominator_doc2)
        SimObj.active_clusters[doc1] = 1
        SimObj.cluster_info[doc1] = doc1

    print "Clustering Documents"
    A = [] #assembles clustering as sequence of merges

    new_cluster = number_of_files + 1 #cluster number
    output = "Document Clustering \n"
    while SimObj.number_of_active_clusters() > 1: # While there exists more than one cluster
        score, c1, c2 = SimObj.get_high_sim() # most similar pair of clusters
        output +=  " %s + %s ---> %s \n" %(c1, c2, new_cluster)
        A.append((c1, c2)) #storing merge sequence
        SimObj.cluster_info[new_cluster] = SimObj.documents(c1) + SimObj.documents(c2)
        SimObj.active_clusters[new_cluster] = len(SimObj.cluster_info[new_cluster])
        for ocluster, active in SimObj.active_clusters.items(): #only for active clusters
            if active != -1:
                SimObj.sim_matrix[new_cluster][ocluster] = SimObj.group_link_avg(new_cluster, ocluster)
        SimObj.active_clusters[c1], SimObj.active_clusters[c2] = -1, -1
        new_cluster += 1
    write_to_file(output, "cluster.txt")
    print "Document Clustering completed. Output written to cluster.txt"

    final_cluster = new_cluster - 1
    cc_documents = SimObj.cluster_info[final_cluster] #get list of documents in the centroid

    m_list = [ (inv_index.cosine_denominator(list_of_files[document]), list_of_files[document]) for document in range(0, number_of_files) ]
    centroid = median(m_list)[1]
    print centroid
    print SimObj.get_high_sim_centroid(centroid, final_cluster)

if __name__ == "__main__":
    if len(sys.argv) > 1:
        arguments = sys.argv[1]
        main(arguments)
    else:
        print "Missing path to input file directory \nUsage: ./cluster input_files"
