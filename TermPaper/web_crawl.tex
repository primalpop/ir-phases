%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original Author:
% Nicolas Diaz (nsdiaz@uc.cl) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images
\usepackage{hyperref}

\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{No Country for Old Web}\\ % Title
Keeping web crawl data updated} % Subtitle

\author{\textsc{Primal Pappachan} % Author
\\{\textit{University of Maryland, Baltimore County}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else

\begin{abstract}

Search engines are one of the most used web applications today. Outlines some of the challenges and goes deep into the the topic of refresh policies related to web crawling. Surveys existing techniques to solve the problem and also verifies how much of challenges mentioned in seminal papers were covered and presents new challenges to be tackled in this domain.

\end{abstract}

\hspace*{3,6mm}\textit{Keywords:} webcrawling , refresh policies , search engine , revisiting policies % Keywords

\vspace{30pt} % Some vertical space between the abstract and first section

%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------

\section*{Introduction}

For many of us a search engine is the starting point of the Internet. We browse the web using Google, Bing, DuckDuckGo or any other search engine to find what we are looking for. In a nutshell, an user interacts with search engine by entering some keywords as a query and receive web pages that contain the keyword and ranked according to relevancy. \\

This is an oversimplified explaination of how the search engine works. The size of web and the frequency at which it changes makes it much more challenging than a simple information retrieval problem. To add some perspective to the problem at hand, 20 million pages were indexed by AltaVista in 1995 when most of the seminal papers related to this topic were written. Fast-forwarding to 2008, 1 trillion URLs were known by Google and Yahoo. Each of web pages contained 10 - 100 KB of textual content and roughly has 100 links per page. \cite{stats2008st} \\ 

Before going into the details of Web Crawling, it is important to understand how a search engine is assembled together. Figure 1 below the schema of a search engine from the works of \cite{arasu2001searching}. As seen in the figure, a search engine relies on a \textit{crawler} which browses the Web on it's behalf. Starting from a set of URLs, this module follows the links and retrieve pages from the Web into a page repository until a stop criterion is met. Crawling operations will be explained in more detail in the following section. \\

\begin{figure} % Inline image example
\begin{center}
\includegraphics[width=1\textwidth]{searchengine.jpg}
\end{center}
\caption{Searching the web}
\end{figure}

The indexer module builds an index by extracting tokens from the pages in the repository. It generates a look up table where words act as keys and the URLs that point to the page they occur in are values. The collection analysis module is responsible for creating various types of utility indexes which provides access to pages with specific structure and content (contains some number of images) \cite{arasu2001searching}. Query engine deals with user requests and uses the indexes to retrieve the URLs of relevant pages from the repository. Ranking module does the task of sorting results so that results near the top are most likely to be of interest to the user. \\

In rest of the paper, we describe in more detail the web crawling aspect of the search engine. This paper serves as an Introduction to the challenges associated with keeping web crawl updated and some interesting techniques that have been developed over the years. \\


\section{Web Crawling}

As mentioned earlier web crawling is a small program that browses the Web on the behalf of search engine. They are given a starting set of urls (seed / initial urls) whose pages they retrieve from the Web. The crawler also extracts the urls appearing in the retrieved pages and adds it to a queue (to visit urls). The retrieved pages are added to a repository and the crawler goes onto visit urls from the to visit queue. \cite{baeza1999modern} 

\begin{figure}[h] % Inline image example
\begin{center}
\includegraphics[width=1\textwidth]{webcrawling.png}
\end{center}
\caption{What is a web crawler?}
\end{figure}

\subsection{Crawling Challenges}

As mentioned earlier, the Web is enormous and changing frequently. Therefore web crawling comes with its unique set of challenges as outlined by \cite{arasu2001searching}.

\begin{enumerate}

\item What pages should the crawler download?

Since the Web is so large, a crawler can only download a small portion of the Internet. It has to carefully select important pages for crawling by prioritizing the urls in to visit queue properly so that portion of the Internet it has visited is significant.

\item How should the crawler refresh pages? 

One of the unique challenges of Information retrieval in the Web is how often the web pages change. Therefore a crawler has to revisit the pages and refresh their contents in the repository so that the local copy of Web maintained by the crawler is not out-of-date with respect to the remote one. In the next section we will be explaining in more detail about challenges involved in maintaining the \textit{freshness} of the collection and techniques to solve them.

\item How to be unobstrusive on the web servers being visited?

When a crawler is working, it consumes resources belonging to other organizations such as disk, CPU resources and network bandwidth. Therefore the crawler should minimize the impact on other web servers so that the administrators of those website would not complain.

\item How to parallize the crawling the process? 

Web crawling is a time consuming process and steps should be taken to reduce this time by running the crawler on multiple machines and parallelizing the downloads of pages. Additionally, these parallel crawlers should be coordinated properly so that they do not interfere with each other's working and does not violate the crawling policy. Coordination can incur communication overhead and therefore limits the number of simultaneous crawlers possible. 

\end{enumerate}

\subsection{Crawler Objective}

According to Coffman et. al \cite{coffman1997optimal}, one of the main objectives of a crawler is to minimize the fraction of time pages remain outdated. This can be split into two different objectives based on two different metrics. 

\begin{itemize}
\item Keep the average freshness of pages high
\item Keep the average age of pages low
\end{itemize}

The cost functions age and freshness are defined as follows by \cite{cho2000synchronizing}.

\begin{figure}[h] % Inline image example
\begin{center}
\includegraphics[width=1\textwidth]{freshness.png}
\end{center}
\caption{Freshness of a page p at time t}
\end{figure}

Freshness represents the fraction of up-to-date pages in the repository. When all pages in the local repository are synchronized with that of the remote repository, freshness is 1. On the other hand, age is a measure of how old the downloaded copies of web pages are. These two metrics are related and similar.

\begin{figure}[h] % Inline image example
\begin{center}
\includegraphics[width=1\textwidth]{age.png}
\end{center}
\caption{Age of a page p at time t}
\end{figure}

\subsection{Crawler Design Issues}

In their seminal paper in 1999 Cho et. al \cite{cho1999evolution} describes about various crawler design issues and their possible trade offs. They identified two types of crawlers with constrasting features so as to tackle the web crawling challenges earlier mentioned.

The first of these is a periodic crawler and their features are

\begin{itemize}
\item Batch mode - periodically updates the entire collection in a single crawl
\item Shadowing - new set of pages are collected and stored in a separate space and replaces the current collection at the end of the crawl
\item Fixed frequency - revisits all pages at same frequency irrespective of how ofthen they change
\end{itemize} 

Secondly, we have the incremental crawler with a constrasting set of features with that of a periodic crawler.

\begin{itemize}

\item Steady mode - crawler runs continously and incrementally updates the collection
\item In place - updates the web pages in place by replacing old version with new version when it is downloaded
\item Variable frequency - optimizes revisits based on how frequently they change 

\end{itemize}

The advantages of these types of crawlers are outlined in the table below.

\begin{table}[h]
\begin{tabular}{|c|c|c|}
\hline 
\textbf{Periodic Crawler} & Easy to implement & High availability \\ 
\hline 
\textbf{Incremental Crawler }& High freshness & Minimal load on network and servers \\ 
\hline
\end{tabular} 
\caption{Periodic versus incremental} 
\end{table}

Architecture of an incremental crawler add the diagram and some explanation

\section{Revisiting policies}

Focusing on the frequency of revisits by a crawler, there are two types of revisiting policies as detailed by \cite{cho2003effective}. The first one called uniform policy is similar to the fixed frequency feature of the periodic crawler. It revisits all pages in the repository with same frequency irrespective of how often these pages change. On the other hand, Proportional policy involves modelling the revisiting policy based on the rate of change of the pages that is visiting frequency is directly proportional to how frequently the pages change. \\

In the same work \cite{cho2003estimating}, they went onto prove that uniform policy bests proportional policy in terms of average freshness. The intuitive explanation behind this surprising result is that web crawlers have limited set of resources and have to limit the number of pages they can crawl in a given time. In case of proportional policy, they end up allocating too many resources to very frequently changing pages. Also, as the freshness of fast changing pages lasts for shorter periods than that of lesser frequently changing pages. Therefore, proportional policy gets bottlenecked by the frequently updating pages and ends up having lower freshness than the uniform policy.

\subsection{Optimal Policy}

Based on the earlier results, Cho and Garcia-Molina proposed that the optimal revisiting policy for a web crawler is neither uniform nor proportional. \cite{cho2003estimating} They also suggested that web pages that change too often should be penalized so that they do not bottleneck the crawling process. The access frequencies of the web pages should be varied sub-linearly with rate of change. They went onto propose that the exponential distribution based on Poisson process is a good model for describing page changes. Latter on, Ipeirotis et al. came up with a statistical approach to discover the parameters which affect the rate of change of the page \cite{ipeirotis2005modeling}.

\section{Future work}

From the literature survey, I came across the following future work as proposed by the authors of seminal papers mentioned above. The crawler and the website being crawled upon has to negotiate on a right crawling policy so that the load of servers and network is minimized and crawler can maintain high average freshness. Either uniform or proportional policy does not take into consideration the quality of a web page in their policy. These leads into giving equal priority to all web pages irrespective of their quality of content which can result in suboptimal policy. Additionally both of these policies does not use an adaptive scheme for estimating access frequencies of web pages. Static policies which are determined at the beginning of a web crawl tend to perform poorly as the rate at which web pages changes varies over time and models outlined in their papers fail to capture this behaviour. Parallelization and distributed crawling  was in its infancy stages during this time and requires a lot of work before going mainstream.

\subsection{Current status}

I surveyed the recent work to learn about the frontiers of web crawling. The first work I came across describes about adding a user centric metric to web crawling policy \cite{pandey2005user}. This would in turn maximize the quality of user experience for those who are using the search engine. By basing the crawling policy on usage patterns such as which keywords are searched, their frequency and the results of interest to the users, the web crawler is able to refresh particular page based on its possible impact to user experience. Pandey and Olson went onto study how prioritizing the refreshment of pages based on the number of times the page appears in top K search results for queries, for some constant K \cite{pandey2008crawl}. \\

\cite{olston2008recrawl} characterizes information longevity as a feature of the web pages itself and identifies it as a key factor in crawler effectiveness. They introduce two different policies based on this feature and through simulated and real web experiments show that these policies obtain better freshness at a lower cost. The sitemap protocol mechanism proposed by Google makes the job of a crawler slightly easier as it tells the crawlers about urls on the website and modification date. It helps crawlers to discover about pages and updates which it otherwise would have missed \cite{google2014sitemap}. 

\subsection{What still needs to be done}


\section{Motivation}

centipede and class project

\section{Conclusion}

\section*{Appendix}

Presentation Link: \url{http://bit.ly/primal676}


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{unsrt}

\bibliography{references}

%----------------------------------------------------------------------------------------

\end{document}