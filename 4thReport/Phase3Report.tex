\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{graphicx,subfigure}
%\usepackage[pdftex]{graphicx}	
\usepackage{url}
\usepackage{soul,color}

%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{Information Retrieval} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge Programming Project - Phase 4 \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont 								\normalsize
        Primal Pappachan\\[-3pt]		\normalsize
        primal1@umbc.edu\\[-3pt]		\normalsize
        \today
}
\date{}


%%% Begin document
\begin{document}
\maketitle
\section{Introduction}
In Phase 4 of the project, I have implemented a retrieval engine which can be used to query a given document collection. I have used my code from earlier phases of the project for tokenizing, calculating normalized term weights as well as building the inverted index. I have used Python to code the entire program. To execute the program from a terminal (after setting right permissions for the file), type 

\begin{verbatim}
$./retrieve wts files <query_term_weights> 
\end{verbatim}

For example
\begin{verbatim}
$./retrieve wts input_files "0.8 international 0.2 affairs"
\end{verbatim}

The first parameter is path to the input files and the second parameter is a string with query term weights and query terms. You need to install the NLTK to run the program. Please refer to the documentation\footnote{\url{http://www.nltk.org/install.html}} on how to install NLTK. Additionally I have used docopt \footnote{\url{http://docopt.org/}} module for parsing multiple command line arguments.

\paragraph{Output}

The output is a ranked list of documents with respect to the query given. The order is based on the document-query similarity scores. Only the first ten documents with highest similarity scores are displayed as a list of tuples with document id and similarity score as first and second elements of the tuple respectively.

\section{Implementation}

I extended the index program from Phase 3 for implementing the retrieval engine. This helped to reuse some of the data structures as term document dictionary which made the task easier. The same data structure could be instructed from postings and dictionary file from Phase 3 if necessary. 

\subsection{Processing}

The retrieval engine takes in queries which are list of words with their term weights and runs the same preprocessing method on them as was used in previous phases. I modified the clean_html() method of tokenizer so that it would be able to handle single words instead of file objects. After this step, it was converted to a dictionary with query terms and weights, for example: query_vector['international'] = 0.8.  

\subsection{Calculating Similarity}

The document query similarity was calculated by taking a dot product between query vector and row corresponding to the document in the term document matrix. The term document matrix was implemented as a nested dictionary. The documents in which a term has appeared can be obtained by retrieving the values corresponding to the key (query term) from the first level of the dictionary (i.e weights[term].keys() where weight is the term document matrix). For each term in the query vector, the documents in which it appears was obtained as earlier mentioned. In the next step, the query term weight was multiplied with corresponding term weight from term document matrix and the results were summed up for all terms in the query vector. This sum was stored in another dictionary with document ids as keys. This operation was repeated for all the documents in which the query term was present

\section{Results}

After the calculation of similarity scores, the documents with non-zero scores were copied over to a list. This list was sorted in descending order based on the similarity score. The first 10 elements of this sorted list is displayed as output.  

\subsection{Complexity of algorithms}



\subsection{Evaluation}

The retrieval engine was tested with the sample queries listed on the project page. The output of queries have been given below

\begin{itemize}

\item diet \\ 

\begin{verbatim}

\end{verbatim}

\item

\end{itemize}


\clearpage

\begin{thebibliography}{1}

\bibitem{impj}Python nested dictionary http://stackoverflow.com/questions/635483/what-is-the-best-way-to-implement-nested-dictionaries-in-python

\bibitem{comp} http://nlp.stanford.edu/IR-book/html/htmledition/postings-file-compression-1.html

\bibitem{tuple} http://stackoverflow.com/questions/16302209/python-string-from-list-comprehension


  \end{thebibliography}

%%% End document
\end{document}