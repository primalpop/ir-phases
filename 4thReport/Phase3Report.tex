\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{graphicx,subfigure}
%\usepackage[pdftex]{graphicx}	
\usepackage{url}
\usepackage{soul,color}

%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{Information Retrieval} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge Programming Project - Phase 4 \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont 								\normalsize
        Primal Pappachan\\[-3pt]		\normalsize
        primal1@umbc.edu\\[-3pt]		\normalsize
        \today
}
\date{}


%%% Begin document
\begin{document}
\maketitle
\section{Introduction}
In Phase 4 of the project, I have implemented a retrieval engine which can be used to query a given document collection. I have used my code from earlier phases of the project for tokenizing, calculating normalized term weights as well as building the inverted index. I have used Python to code the entire program. To execute the program from a terminal (after setting right permissions for the file), type 

\begin{verbatim}
$./retrieve wts files <query_term_weights> 
\end{verbatim}

For example
\begin{verbatim}
$./retrieve wts input_files "0.8 international 0.2 affairs"
\end{verbatim}

The first parameter is path to the input files and the second parameter is a string with query term weights and query terms. You need to install the NLTK to run the program. Please refer to the documentation\footnote{\url{http://www.nltk.org/install.html}} on how to install NLTK. Additionally I have used docopt \footnote{\url{http://docopt.org/}} module for parsing the commandline arguments.

\paragraph{Output}


\section{Preprocessing and Term weighting}

I used preprocessing code from previous phase albeit with two minor additional constraints on validity of tokens. The size of token should be between 2 and 20 and it should not be a number. For term weighting, instead of writing the results to a file I stored it in a nested hashtable for easier retrieval. The normalized weight of a term could be retrieved using a simple dictionary operation (weights[document][term], where weights is the name of the dictionary).

\section{Building the index}

The index was built by iterating through the nested dictionary. For each term, the id of document which it appears in as well as normalized weight of the term is written to the postings file. A counter variable was used to record the last line number written in file. After going through all documents corresponding to a term, a single entry for that term is stored in a list as tuple (term, document frequency, location of first record in postings file). The document frequency for a term was obtained from the inverse document dictionary (inverse\_doc\_freq[term] returns number of documents that contain that term) built in phase 2. After going through all the tokens, the list of tuples were sorted according in alphabetical order of terms and the result was written to the dictionary file.      

\section{Evaluation}

I ran index on a varying number of documents and plotted a graph of indexing time as a function of the number of documents. The timing was done for the complete process, i.e starting with the raw input files to the generation of two output files. For experiments, I used a macbook air with 8 gb of ram. \\

In the first graph, the real time (elapsed time) taken by the program increases linearly with number of files (except for a slight decrease for 100 files which happened because randomly chosen list of files in that step were smaller in size compared to earlier step as can be seen from size of files graph). With larger number of files (100, 200, 300, 400, 500) the time grows linearly  as well. The system time remains fairly constant throughout the first graph where as it registers a slight increase in the second graph.  \\


\begin{figure}[h]
\centering
\subfigure[10, 20, 40, 80, 100]{ \label{fig:sub1}\includegraphics[width=.6\linewidth]{time_1.png}}
\subfigure[100, 200, 300, 400, 500]{ \label{fig:sub2}\includegraphics[width=.6\linewidth]{time_2.png}}
\caption{Execution time of index against number of documents}
\end{figure}

Additionally, I also compared the output file sizes against set of input files for different number of files. I used the python function os.path.getsize which returns the file size in bytes which was converted into MB by dividing by 1024*1024. \\

As you can see from the graphs below, the size of the postings file was comparable to that of the input document collection. The size of dictionary was much smaller in comparison. For compressing the postings file, I could use a smaller representation of the document id such as hash value or the just the filename instead of using the entire path for each file which on the flip side makes it easier to retrieve the document. Additionally I could use techniques like variable encoding (variable byte codes, gamma codes)  \cite{comp} for compressing the postings file.   


\begin{figure}[h]
\centering
\subfigure[10, 20, 40, 80, 100]{ \label{fig:sub1}\includegraphics[width=.6\linewidth]{size_1.png}}
\subfigure[100, 200, 300, 400, 500]{ \label{fig:sub2}\includegraphics[width=.6\linewidth]{size_2.png}}
\caption{Size of files against number of documents}
\end{figure}

\clearpage

\begin{thebibliography}{1}

\bibitem{impj}Python nested dictionary http://stackoverflow.com/questions/635483/what-is-the-best-way-to-implement-nested-dictionaries-in-python

\bibitem{comp} http://nlp.stanford.edu/IR-book/html/htmledition/postings-file-compression-1.html

\bibitem{tuple} http://stackoverflow.com/questions/16302209/python-string-from-list-comprehension


  \end{thebibliography}

%%% End document
\end{document}